from typing import List, Optional, Any, Dict
from datetime import datetime
import json
import uuid
from ...server import mcp
from ..config import config
from ..storage import StorageValidation
from ..indexer import indexer

@mcp.tool()
def append_daily_log(
    content: Optional[str] = None, 
    atomic_fact: Optional[Dict[str, Any]] = None,
    tags: Optional[List[str]] = None,
    scope: Optional[str] = None
) -> str:
    """
    **å†™å…¥è®°å¿†** - å½“éœ€è¦ä¿å­˜ä¿¡æ¯æ—¶è°ƒç”¨æ­¤å·¥å…·ã€‚
    
    ## è§¦å‘æ—¶æœº (WHEN TO CALL)
    å½“ç”¨æˆ·è¯´å‡ºä»¥ä¸‹å…³é”®è¯æ—¶ï¼Œ**å¿…é¡»**è°ƒç”¨æ­¤å·¥å…·ï¼š
    - "è®°ä½", "ä¿å­˜", "è®°å½•", "åˆ«å¿˜äº†", "ä»¥åéƒ½è¿™æ ·"
    - "æˆ‘å–œæ¬¢...", "æˆ‘ä¸å–œæ¬¢...", "æˆ‘ä¹ æƒ¯..."
    - "è¿™ä¸ªé¡¹ç›®ç”¨...", "è¿™ä¸ªä»“åº“çš„è§„èŒƒæ˜¯..."
    - ä»»åŠ¡å®Œæˆæ—¶ä¸»åŠ¨è®°å½•è¿›åº¦
    - è§£å†³é—®é¢˜åè®°å½•è§£å†³æ–¹æ¡ˆ
    
    ## å‚æ•°ä½¿ç”¨æŒ‡å—
    
    ### 1. æ¯æ—¥ç¬”è®° (çŸ­æœŸ/ä¸´æ—¶)
    ç”¨äºä»»åŠ¡è¿›åº¦ã€ä¸´æ—¶æƒ³æ³•ã€é”™è¯¯è®°å½•ï¼š
    ```
    append_daily_log(content="å®Œæˆäº†ç”¨æˆ·è®¤è¯æ¨¡å—çš„é‡æ„")
    ```
    
    ### 2. é¢†åŸŸçŸ¥è¯† (é•¿æœŸ)
    ç”¨äºæŠ€æœ¯è§„èŒƒã€API ç”¨æ³•ã€æœ€ä½³å®è·µï¼š
    ```
    append_daily_log(atomic_fact={
        "fact": "Next.js 15 ä½¿ç”¨ App Router ä½œä¸ºé»˜è®¤è·¯ç”±",
        "category": "domain_knowledge"
    })
    ```
    
    ### 3. ç”¨æˆ·åå¥½ (æ°¸ä¹…)
    ç”¨äºç”¨æˆ·ä¹ æƒ¯ã€é£æ ¼åå¥½ã€å·¥ä½œæ–¹å¼ï¼š
    ```
    append_daily_log(atomic_fact={
        "fact": "ç”¨æˆ·å–œæ¬¢ä½¿ç”¨ Tailwind CSS",
        "category": "user_preference"
    })
    ```
    
    ### 4. é¡¹ç›®ç‰¹å®šçŸ¥è¯† (scope)
    å½“ç”¨æˆ·è¯´"åœ¨è¿™ä¸ªé¡¹ç›®ä¸­"ã€"è¿™ä¸ªä»“åº“"æ—¶ä½¿ç”¨ scopeï¼š
    ```
    append_daily_log(
        atomic_fact={"fact": "ä½¿ç”¨ vanilla CSS", "category": "user_preference"},
        scope="project:landing-page"
    )
    ```
    
    **æ³¨æ„**: ä¸è¦è¯¢é—®æ—¥æœŸï¼Œç³»ç»Ÿè‡ªåŠ¨è®°å½•æ—¶é—´æˆ³ã€‚
    """
    now = datetime.now()
    
    # 1. Handle Daily Log (Ephemeral)
    if content:
        log_path = StorageValidation.get_daily_log_path(now)
        # Create header if new file
        if not log_path.exists() or log_path.stat().st_size == 0:
            header_tags = str(tags) if tags else "[]"
            header = f"---\ntype: daily_log\ndate: \"{now.strftime('%Y-%m-%d')}\"\ntags: {header_tags}\n---\n\n"
            StorageValidation.append_to_file(log_path, header)
            
        time_str = now.strftime("%H:%M")
        entry = f"### {time_str}\n{content}"
        StorageValidation.append_to_file(log_path, entry)
        # Trigger Re-index
        indexer.build_index()
        return f"Appended log to {log_path}"

    # 2. Handle Knowledge Graph (Atomic Fact)
    if atomic_fact:
        # Validate atomic_fact structure
        fact_content = atomic_fact.get("fact")
        if not fact_content or not isinstance(fact_content, str):
            return "Error: atomic_fact must contain a non-empty 'fact' string field."
        
        category = atomic_fact.get("category", "general")
        # Define storage strategies based on category
        if category == "user_preference":
            target_file = config.storage_path / "MEMORY.md"
            fact_str = f"- {fact_content}"
            if scope and scope != "global":
                fact_str = f"- [{scope}] {fact_content}"
            StorageValidation.append_to_file(target_file, f"\n{fact_str}")
            return "Updated User Profile in MEMORY.md"
        
        else: # Domain Knowledge
            target_dir = config.storage_path / "knowledge" / "areas" / "general"
            target_dir.mkdir(parents=True, exist_ok=True)
            target_file = target_dir / "items.json"
            
            # Load existing items
            items = []
            if target_file.exists():
                try:
                    items = json.loads(target_file.read_text(encoding="utf-8"))
                except Exception:
                    items = []
            
            # Knowledge Evolution Logic
            supersedes_id = atomic_fact.get("supersedes_id") or atomic_fact.get("supersededBy")
            
            # Generate ID if missing
            if "id" not in atomic_fact:
                atomic_fact["id"] = f"fact-{uuid.uuid4().hex[:8]}"
            
            atomic_fact["timestamp"] = now.isoformat()
            atomic_fact["status"] = "active"
            atomic_fact["scope"] = scope or "global"  # Default to global
            
            if supersedes_id:
                # Mark old item as superseded
                updated_count = 0
                for item in items:
                    if item.get("id") == supersedes_id and item.get("status") == "active":
                        item["status"] = "superseded"
                        item["supersededBy"] = atomic_fact["id"]
                        updated_count += 1
                if updated_count == 0:
                    return f"Warning: Could not find active fact with ID {supersedes_id} to supersede."
            
            items.append(atomic_fact)
            
            target_file.write_text(json.dumps(items, indent=2, ensure_ascii=False), encoding="utf-8")
            return f"Added Atomic Fact {atomic_fact['id']} (scope: {atomic_fact['scope']}) to {target_file}"

    return "No content or fact provided."

@mcp.tool()
def query_knowledge(scope: Optional[str] = None, category: Optional[str] = None) -> str:
    """
    **çŸ¥è¯†åº“æŸ¥è¯¢** - ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å·²ä¿å­˜çš„çŸ¥è¯†æ¡ç›®ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    - ç”¨æˆ·é—® "æˆ‘çš„åå¥½æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼ˆæŸ¥è¯¢ user_preferenceï¼‰
    - ç”¨æˆ·é—® "ä¹‹å‰è®°å½•çš„æŠ€æœ¯è§„èŒƒæœ‰å“ªäº›ï¼Ÿ"ï¼ˆæŸ¥è¯¢ domain_knowledgeï¼‰
    - åœ¨ç‰¹å®šé¡¹ç›®ä¸­å·¥ä½œæ—¶ï¼ŒæŸ¥è¯¢è¯¥é¡¹ç›®çš„ä¸“å±é…ç½®
    
    ## å‚æ•°è¯´æ˜
    
    ### scope (ä½œç”¨åŸŸè¿‡æ»¤)
    - `None`: è¿”å›å…¨å±€çŸ¥è¯† + å½“å‰é¡¹ç›®çš„çŸ¥è¯†
    - `'global'`: ä»…è¿”å›å…¨å±€çŸ¥è¯†
    - `'project:my-app'`: ä»…è¿”å›è¯¥é¡¹ç›®çš„ä¸“å±çŸ¥è¯† + å…¨å±€çŸ¥è¯†
    
    ### category (åˆ†ç±»è¿‡æ»¤)
    - `'domain_knowledge'`: æŠ€æœ¯è§„èŒƒã€API ç”¨æ³•ã€æœ€ä½³å®è·µ
    - `'user_preference'`: ç”¨æˆ·åå¥½ã€ä¹ æƒ¯ã€é£æ ¼
    - `None`: è¿”å›æ‰€æœ‰åˆ†ç±»
    
    ## è¿”å›æ ¼å¼
    æ¯æ¡çŸ¥è¯†æ˜¾ç¤ºï¼šä½œç”¨åŸŸæ ‡ç­¾ï¼ˆå¦‚æœ‰ï¼‰ã€çŸ¥è¯†å†…å®¹ã€ID
    
    ## ä¸ append_daily_log çš„å…³ç³»
    - `append_daily_log` å†™å…¥çŸ¥è¯†
    - `query_knowledge` è¯»å–çŸ¥è¯†
    """
    target_file = config.storage_path / "knowledge" / "areas" / "general" / "items.json"
    
    if not target_file.exists():
        return "No knowledge base found."
    
    try:
        items = json.loads(target_file.read_text(encoding="utf-8"))
    except Exception:
        return "Error reading knowledge base."
    
    # Filter by status
    active_items = [i for i in items if i.get("status") == "active"]
    
    # Filter by scope
    if scope:
        if scope == "global":
            active_items = [i for i in active_items if i.get("scope", "global") == "global"]
        else:
            # Include global + specific scope
            active_items = [i for i in active_items if i.get("scope", "global") in ["global", scope]]
    
    # Filter by category
    if category:
        active_items = [i for i in active_items if i.get("category") == category]
    
    if not active_items:
        return "No matching knowledge found."
    
    # Format output
    output = []
    for item in active_items:
        scope_tag = f"[{item.get('scope', 'global')}]" if item.get('scope') != 'global' else ""
        output.append(f"- {scope_tag} {item.get('fact')} (id: {item.get('id')})")
    
    return "\n".join(output)

@mcp.tool()
def get_period_context(period: str, date: Optional[str] = None) -> str:
    """
    **å‘¨æœŸç´¢å¼•** - è·å–æŒ‡å®šæ—¶é—´æ®µçš„æ—¥å¿—æ‘˜è¦å’Œæ–‡ä»¶ç´¢å¼•ï¼Œç”¨äºç”Ÿæˆå‘¨æŠ¥/æœˆæŠ¥ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    å½“ç”¨æˆ·è¯´ï¼š
    - "å¸®æˆ‘å†™ä¸€ä»½å‘¨æŠ¥"
    - "æ€»ç»“ä¸€ä¸‹è¿™ä¸ªæœˆåšäº†ä»€ä¹ˆ"
    - "å›é¡¾ä¸Šå‘¨çš„å·¥ä½œ"
    
    ## å·¥ä½œæµç¨‹
    1. è°ƒç”¨ `get_period_context(period='week')` è·å–ç´¢å¼•
    2. é˜…è¯»æ‘˜è¦ï¼Œäº†è§£æ¯å¤©çš„æ¦‚å†µ
    3. å¦‚éœ€è¯¦ç»†å†…å®¹ï¼Œè°ƒç”¨ `read_memory_content` æŒ‰éœ€åŠ è½½
    4. æ’°å†™æ€»ç»“æŠ¥å‘Š
    5. è°ƒç”¨ `archive_period` ä¿å­˜æ€»ç»“
    
    ## å‚æ•°è¯´æ˜
    - `period`: æ—¶é—´å‘¨æœŸï¼Œ`'week'` æˆ– `'month'`
    - `date`: å¯é€‰ï¼ŒæŒ‡å®šæ—¥æœŸ (æ ¼å¼ YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
    
    ## è¿”å›æ ¼å¼
    è¿”å›ç®€çŸ­æ‘˜è¦ + æ–‡ä»¶è·¯å¾„ç´¢å¼•ï¼Œä¸è¿”å›å®Œæ•´å†…å®¹ï¼š
    ```
    ğŸ“… 2026-02-03 | 3 entries | path/to/file.md
       æ‘˜è¦: å®Œæˆäº†ç”¨æˆ·è®¤è¯æ¨¡å—...
    ```
    
    ## æŒ‰éœ€åŠ è½½
    å¯¹äºéœ€è¦è¯¦ç»†äº†è§£çš„æ—¥æœŸï¼Œä½¿ç”¨è¿”å›çš„æ–‡ä»¶è·¯å¾„è°ƒç”¨ `read_memory_content`
    """
    from datetime import timedelta
    import re
    
    target_date = datetime.now()
    if date:
        try:
            target_date = datetime.strptime(date, "%Y-%m-%d")
        except Exception:
            return f"Error: Invalid date format {date}. Use YYYY-MM-DD"
    
    start_date = target_date
    end_date = target_date
    
    if period == "week":
        start_date = target_date - timedelta(days=target_date.weekday())
        end_date = start_date + timedelta(days=6)
    elif period == "month":
        start_date = target_date.replace(day=1)
        if start_date.month == 12:
            next_month = start_date.replace(year=start_date.year+1, month=1)
        else:
            next_month = start_date.replace(month=start_date.month+1)
        end_date = next_month - timedelta(days=1)
    else:
        return "Error: Period must be 'week' or 'month'"
    
    # Build index with summaries
    index_parts = []
    index_parts.append(f"# {period.upper()} æ¦‚è§ˆ")
    index_parts.append(f"**æ—¶é—´èŒƒå›´**: {start_date.strftime('%Y-%m-%d')} â†’ {end_date.strftime('%Y-%m-%d')}")
    index_parts.append("")
    
    file_paths = []
    current = start_date
    while current <= end_date:
        log_path = StorageValidation.get_daily_log_path(current)
        if log_path.exists():
            content = log_path.read_text(encoding="utf-8")
            
            # Count entries (### HH:MM headers)
            entry_count = len(re.findall(r'^### \d{2}:\d{2}', content, re.MULTILINE))
            
            # Extract first 100 chars of actual content (skip YAML header)
            content_body = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
            summary = content_body.strip()[:100].replace('\n', ' ')
            if len(content_body) > 100:
                summary += "..."
            
            index_parts.append(f"ğŸ“… **{current.strftime('%Y-%m-%d')}** | {entry_count} entries")
            index_parts.append(f"   ğŸ“„ `{log_path}`")
            index_parts.append(f"   æ‘˜è¦: {summary}")
            index_parts.append("")
            file_paths.append(str(log_path))
        
        current += timedelta(days=1)
    
    if not file_paths:
        return "No logs found for this period."
    
    index_parts.append("---")
    index_parts.append(f"**å…± {len(file_paths)} ä¸ªæ—¥å¿—æ–‡ä»¶**")
    index_parts.append("å¦‚éœ€è¯¦ç»†å†…å®¹ï¼Œè¯·è°ƒç”¨ `read_memory_content` å¹¶ä¼ å…¥ä¸Šè¿°æ–‡ä»¶è·¯å¾„ã€‚")
    
    return "\n".join(index_parts)

@mcp.tool()
def archive_period(summary_content: str, period: str) -> str:
    """
    **ä¿å­˜å‘¨æœŸæ€»ç»“** - å°†æ’°å†™å¥½çš„å‘¨æŠ¥/æœˆæŠ¥ä¿å­˜åˆ°æ°¸ä¹…æ–‡ä»¶ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    åœ¨ä½¿ç”¨ `get_period_context` è·å–æ•°æ®å¹¶æ’°å†™æ€»ç»“åï¼Œè°ƒç”¨æ­¤å·¥å…·ä¿å­˜ã€‚
    
    ## å·¥ä½œæµç¨‹
    1. `get_period_context(period='week')` - è·å–åŸå§‹æ•°æ®
    2. é˜…è¯»æ•°æ®ï¼Œæ’°å†™ç²¾ç‚¼æ€»ç»“
    3. `archive_period(summary_content='...', period='week')` - ä¿å­˜
    
    ## å‚æ•°è¯´æ˜
    - `summary_content`: ä½ æ’°å†™çš„æ€»ç»“å†…å®¹ (Markdown æ ¼å¼)
    - `period`: æ—¶é—´å‘¨æœŸï¼Œ`'week'` æˆ– `'month'`
    
    ## ä¿å­˜ä½ç½®
    æ–‡ä»¶ä¿å­˜åˆ°: `memory/{period}_summary_{date}.md`
    
    ## æ€»ç»“å»ºè®®æ ¼å¼
    ```markdown
    # å‘¨æŠ¥ 2026-02-06
    
    ## å®Œæˆäº‹é¡¹
    - äº‹é¡¹ 1
    - äº‹é¡¹ 2
    
    ## é‡åˆ°çš„é—®é¢˜
    - é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ
    
    ## ä¸‹å‘¨è®¡åˆ’
    - å¾…åŠäº‹é¡¹
    ```
    """
    now = datetime.now()
    path = config.storage_path / "memory" / f"{period}_summary_{now.strftime('%Y-%m-%d')}.md"
    path.write_text(summary_content, encoding="utf-8")
    return f"Archived {period} summary to {path}"

