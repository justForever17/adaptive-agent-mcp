"""
çŸ¥è¯†å›¾è°±å·¥å…· - å®ä½“å…³ç³»ç®¡ç†ä¸æŸ¥è¯¢

æä¾›:
- å®ä½“æŠ½å– (ä»æ–‡æœ¬æå–ä¸‰å…ƒç»„)
- å…³ç³»æŸ¥è¯¢ (æŸ¥è¯¢å®ä½“çš„å…³è”)
- å›¾ç»Ÿè®¡ä¿¡æ¯
"""

import re
from typing import List, Optional, Dict, Any
from ...server import mcp
from ..config import config
from ..graph_store import (
    GraphStore, get_graph_store, 
    Triple, Entity
)


# å¸¸è§çš„è°“è¯æ¨¡å¼ (ç”¨äºç®€å•è§„åˆ™æå–)
PREDICATE_PATTERNS = {
    r"å–œæ¬¢|åå¥½|prefer|like": "LIKES",
    r"ä½¿ç”¨|ç”¨|use": "USES",
    r"ä¸å–œæ¬¢|è®¨åŒ|dislike|hate": "DISLIKES",
    r"æ“…é•¿|ç²¾é€š|expert|master": "SKILLED_AT",
    r"å­¦ä¹ |åœ¨å­¦|learning": "LEARNING",
    r"å·¥ä½œäº|åœ¨.*å·¥ä½œ|work.*at|work.*on": "WORKS_ON",
    r"åˆ›å»º|åˆ›é€ |create|build": "CREATED",
    r"å±äº|æ˜¯.*çš„ä¸€éƒ¨åˆ†|belong|part of": "BELONGS_TO",
    r"ä¾èµ–|åŸºäº|depend|based on": "DEPENDS_ON",
}

# å¸¸è§å®ä½“ç±»å‹æç¤ºè¯
ENTITY_TYPE_HINTS = {
    "user": ["æˆ‘", "ç”¨æˆ·", "user", "è‡ªå·±"],
    "technology": [
        "react", "vue", "angular", "next.js", "nuxt", "svelte",
        "python", "javascript", "typescript", "java", "go", "rust",
        "tailwind", "css", "html", "node.js", "deno", "bun",
        "docker", "kubernetes", "aws", "gcp", "azure",
        "postgresql", "mysql", "mongodb", "redis", "sqlite",
    ],
    "framework": ["fastapi", "django", "flask", "express", "nestjs", "spring"],
    "tool": ["vscode", "vim", "git", "npm", "pnpm", "yarn"],
    "concept": ["dark mode", "æš—è‰²æ¨¡å¼", "ç®€æ´", "minimal", "æ€§èƒ½", "performance"],
}


def infer_entity_type(name: str) -> str:
    """æ¨æ–­å®ä½“ç±»å‹"""
    name_lower = name.lower().strip()
    
    for entity_type, hints in ENTITY_TYPE_HINTS.items():
        for hint in hints:
            if hint.lower() in name_lower or name_lower in hint.lower():
                return entity_type
    
    return "concept"


def extract_triples_simple(text: str) -> List[Triple]:
    """
    ç®€å•è§„åˆ™æå–ä¸‰å…ƒç»„ (ä¸ä¾èµ–å¤–éƒ¨ LLM)
    
    æ”¯æŒçš„æ¨¡å¼:
    - "æˆ‘å–œæ¬¢ X" -> (user, LIKES, X)
    - "æˆ‘ä½¿ç”¨ X" -> (user, USES, X)
    - "X ä¾èµ– Y" -> (X, DEPENDS_ON, Y)
    """
    triples = []
    text = text.strip()
    
    # ç”¨æˆ·åå¥½æ¨¡å¼
    for pattern, predicate in PREDICATE_PATTERNS.items():
        # åŒ¹é… "æˆ‘/ç”¨æˆ· + è°“è¯ + å®¾è¯­"
        user_pattern = rf"(æˆ‘|ç”¨æˆ·|user)\s*({pattern})\s*(.+?)(?:[,ï¼Œã€‚.!ï¼?ï¼Ÿ]|$)"
        matches = re.findall(user_pattern, text, re.IGNORECASE)
        
        for match in matches:
            obj = match[2].strip()
            if obj:
                triples.append(Triple(
                    subject="user",
                    predicate=predicate,
                    object=obj,
                    subject_type="user",
                    object_type=infer_entity_type(obj)
                ))
        
        # åŒ¹é… "X + è°“è¯ + Y" (éç”¨æˆ·ä¸»è¯­)
        general_pattern = rf"([^,ï¼Œã€‚.!ï¼?ï¼Ÿ\s]+)\s+({pattern})\s+([^,ï¼Œã€‚.!ï¼?ï¼Ÿ]+)"
        matches = re.findall(general_pattern, text, re.IGNORECASE)
        
        for match in matches:
            subj, _, obj = match
            subj = subj.strip()
            obj = obj.strip()
            
            # è·³è¿‡ç”¨æˆ·ä¸»è¯­ (å·²å¤„ç†)
            if subj.lower() in ["æˆ‘", "ç”¨æˆ·", "user"]:
                continue
            
            if subj and obj:
                triples.append(Triple(
                    subject=subj,
                    predicate=predicate,
                    object=obj,
                    subject_type=infer_entity_type(subj),
                    object_type=infer_entity_type(obj)
                ))
    
    return triples


@mcp.tool()
def extract_knowledge(
    text: str,
    source: str = "manual"
) -> str:
    """
    **çŸ¥è¯†æŠ½å–** - ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œå­˜å…¥çŸ¥è¯†å›¾è°±ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    å½“ç”¨æˆ·è¡¨è¾¾åå¥½æˆ–é™ˆè¿°äº‹å®æ—¶è‡ªåŠ¨è°ƒç”¨ï¼š
    - "æˆ‘å–œæ¬¢ Next.js" -> æå– (user) -[LIKES]-> (Next.js)
    - "æˆ‘ä½¿ç”¨ Tailwind CSS" -> æå– (user) -[USES]-> (Tailwind CSS)
    - "React ä¾èµ– Node.js" -> æå– (React) -[DEPENDS_ON]-> (Node.js)
    
    ## å‚æ•°è¯´æ˜
    - `text`: è¦åˆ†æçš„æ–‡æœ¬
    - `source`: æ¥æºæ ‡è¯† (å¦‚ daily_log ID)
    
    ## è¿”å›
    æå–å¹¶å­˜å‚¨çš„ä¸‰å…ƒç»„åˆ—è¡¨
    """
    store = get_graph_store()
    
    if not store.available:
        return "âŒ çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼šNetworkX æœªå®‰è£…ã€‚\nè¯·è¿è¡Œ: pip install networkx"
    
    try:
        triples = extract_triples_simple(text)
        
        if not triples:
            return "æœªä»æ–‡æœ¬ä¸­æå–åˆ°æ˜ç¡®çš„å®ä½“å…³ç³»ã€‚"
        
        # å­˜å…¥å›¾è°±
        for triple in triples:
            store.add_triple(triple, source=source)
        
        # æ ¼å¼åŒ–è¾“å‡º
        output = [f"âœ… å·²æå–å¹¶å­˜å‚¨ {len(triples)} æ¡å…³ç³»:\n"]
        
        for t in triples:
            output.append(
                f"  ({t.subject}:{t.subject_type}) "
                f"-[{t.predicate}]-> "
                f"({t.object}:{t.object_type})"
            )
        
        return "\n".join(output)
        
    except Exception as e:
        return f"âŒ çŸ¥è¯†æŠ½å–å¤±è´¥: {str(e)}"


@mcp.tool()
def add_knowledge_relation(
    subject: str,
    predicate: str,
    object: str,
    subject_type: str = "concept",
    object_type: str = "concept"
) -> str:
    """
    **æ·»åŠ å…³ç³»** - æ‰‹åŠ¨æ·»åŠ å®ä½“å…³ç³»åˆ°çŸ¥è¯†å›¾è°±ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    - æ‰‹åŠ¨è®°å½•ç”¨æˆ·åå¥½
    - å»ºç«‹æŠ€æœ¯æ ˆå…³è”
    - åˆ›å»ºé¡¹ç›®ä¾èµ–å…³ç³»
    
    ## å‚æ•°è¯´æ˜
    - `subject`: ä¸»è¯­å®ä½“åç§°
    - `predicate`: å…³ç³»ç±»å‹ (LIKES, USES, DEPENDS_ON, WORKS_ON, ...)
    - `object`: å®¾è¯­å®ä½“åç§°
    - `subject_type`: ä¸»è¯­ç±»å‹ (user, technology, concept, project)
    - `object_type`: å®¾è¯­ç±»å‹
    
    ## å¸¸ç”¨è°“è¯
    | è°“è¯ | å«ä¹‰ | ç¤ºä¾‹ |
    |------|------|------|
    | LIKES | å–œæ¬¢ | (user)-[LIKES]->(React) |
    | USES | ä½¿ç”¨ | (user)-[USES]->(VSCode) |
    | DISLIKES | ä¸å–œæ¬¢ | (user)-[DISLIKES]->(Java) |
    | SKILLED_AT | æ“…é•¿ | (user)-[SKILLED_AT]->(Python) |
    | DEPENDS_ON | ä¾èµ– | (Next.js)-[DEPENDS_ON]->(React) |
    """
    store = get_graph_store()
    
    if not store.available:
        return "âŒ çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼šNetworkX æœªå®‰è£…ã€‚"
    
    try:
        triple = Triple(
            subject=subject,
            predicate=predicate,
            object=object,
            subject_type=subject_type,
            object_type=object_type
        )
        
        store.add_triple(triple, source="manual")
        
        return (
            f"âœ… å·²æ·»åŠ å…³ç³»:\n"
            f"  ({subject}:{subject_type}) -[{predicate.upper()}]-> ({object}:{object_type})"
        )
        
    except Exception as e:
        return f"âŒ æ·»åŠ å¤±è´¥: {str(e)}"


@mcp.tool()
def query_knowledge_graph(
    entity: Optional[str] = None,
    predicate: Optional[str] = None,
    entity_type: Optional[str] = None
) -> str:
    """
    **æŸ¥è¯¢çŸ¥è¯†å›¾è°±** - æŸ¥è¯¢å®ä½“å…³ç³»ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    - "ç”¨æˆ·å–œæ¬¢ä»€ä¹ˆæ¡†æ¶ï¼Ÿ" -> query_knowledge_graph(entity="user", predicate="LIKES")
    - "æœ‰å“ªäº›æŠ€æœ¯å®ä½“ï¼Ÿ" -> query_knowledge_graph(entity_type="technology")
    - "Next.js çš„æ‰€æœ‰å…³ç³»" -> query_knowledge_graph(entity="next.js")
    
    ## å‚æ•°è¯´æ˜
    - `entity`: æŸ¥è¯¢ç‰¹å®šå®ä½“çš„å…³ç³» (å¯é€‰)
    - `predicate`: è¿‡æ»¤ç‰¹å®šå…³ç³»ç±»å‹ (å¯é€‰)
    - `entity_type`: è¿‡æ»¤å®ä½“ç±»å‹ (å¯é€‰)
    
    ## è¿”å›
    åŒ¹é…çš„å®ä½“å’Œå…³ç³»åˆ—è¡¨
    """
    store = get_graph_store()
    
    if not store.available:
        return "âŒ çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼šNetworkX æœªå®‰è£…ã€‚"
    
    try:
        output = []
        
        if entity:
            # æŸ¥è¯¢ç‰¹å®šå®ä½“çš„å…³ç³»
            entity_id = entity.lower().replace(" ", "_")
            entity_info = store.get_entity(entity_id)
            
            if entity_info:
                output.append(f"ğŸ“Œ **å®ä½“: {entity_info.name}** (ç±»å‹: {entity_info.type})\n")
            
            # å‡ºè¾¹å…³ç³»
            neighbors = store.query_entity_neighbors(entity_id, predicate, direction="out")
            if neighbors:
                output.append("**å‡ºè¾¹å…³ç³» (->):**")
                for neighbor_id, pred, attrs in neighbors:
                    output.append(f"  -[{pred}]-> {attrs.get('name', neighbor_id)} ({attrs.get('type', 'unknown')})")
            
            # å…¥è¾¹å…³ç³»
            neighbors_in = store.query_entity_neighbors(entity_id, predicate, direction="in")
            if neighbors_in:
                output.append("\n**å…¥è¾¹å…³ç³» (<-):**")
                for neighbor_id, pred, attrs in neighbors_in:
                    output.append(f"  <-[{pred}]- {attrs.get('name', neighbor_id)} ({attrs.get('type', 'unknown')})")
            
            if not neighbors and not neighbors_in:
                output.append("æœªæ‰¾åˆ°ç›¸å…³å…³ç³»ã€‚")
        
        elif entity_type:
            # æŒ‰ç±»å‹æŸ¥è¯¢å®ä½“
            entities = store.get_all_entities(entity_type)
            
            if entities:
                output.append(f"ğŸ“‹ **ç±»å‹ '{entity_type}' çš„å®ä½“ ({len(entities)} ä¸ª):**\n")
                for e in entities:
                    output.append(f"  â€¢ {e.name} ({e.id})")
            else:
                output.append(f"æœªæ‰¾åˆ°ç±»å‹ä¸º '{entity_type}' çš„å®ä½“ã€‚")
        
        else:
            # è¿”å›å›¾ç»Ÿè®¡
            stats = store.stats()
            output.append("ğŸ“Š **çŸ¥è¯†å›¾è°±ç»Ÿè®¡**\n")
            output.append(f"å®ä½“æ€»æ•°: {stats['entity_count']}")
            output.append(f"å…³ç³»æ€»æ•°: {stats['relation_count']}")
            
            if stats['entity_types']:
                output.append("\n**å®ä½“ç±»å‹åˆ†å¸ƒ:**")
                for t, count in sorted(stats['entity_types'].items(), key=lambda x: -x[1]):
                    output.append(f"  â€¢ {t}: {count}")
            
            if stats['predicate_types']:
                output.append("\n**å…³ç³»ç±»å‹åˆ†å¸ƒ:**")
                for p, count in sorted(stats['predicate_types'].items(), key=lambda x: -x[1]):
                    output.append(f"  â€¢ {p}: {count}")
        
        return "\n".join(output) if output else "çŸ¥è¯†å›¾è°±ä¸ºç©ºã€‚"
        
    except Exception as e:
        return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"


@mcp.tool()
def multi_hop_query(
    start_entity: str,
    path: str
) -> str:
    """
    **å¤šè·³æŸ¥è¯¢** - æ²¿å…³ç³»è·¯å¾„è¿›è¡Œæ¨ç†æŸ¥è¯¢ã€‚
    
    ## ä½¿ç”¨åœºæ™¯
    - "ç”¨æˆ·å–œæ¬¢çš„æŠ€æœ¯ä¾èµ–ä»€ä¹ˆï¼Ÿ" -> multi_hop_query("user", "LIKES->DEPENDS_ON")
    - "å’Œç”¨æˆ·ä¸€èµ·å·¥ä½œçš„äººå–œæ¬¢ä»€ä¹ˆï¼Ÿ" -> multi_hop_query("user", "WORKS_WITH->LIKES")
    
    ## å‚æ•°è¯´æ˜
    - `start_entity`: èµ·å§‹å®ä½“åç§°
    - `path`: å…³ç³»è·¯å¾„ (ç”¨ -> åˆ†éš”ï¼Œå¦‚ "LIKES->DEPENDS_ON")
    
    ## è¿”å›
    æ‰€æœ‰åŒ¹é…çš„è·¯å¾„ç»ˆç‚¹
    """
    store = get_graph_store()
    
    if not store.available:
        return "âŒ çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼šNetworkX æœªå®‰è£…ã€‚"
    
    try:
        start_id = start_entity.lower().replace(" ", "_")
        predicates = [p.strip().upper() for p in path.split("->")]
        
        paths = store.multi_hop_query(start_id, predicates)
        
        if not paths:
            return f"æœªæ‰¾åˆ°ä» '{start_entity}' æ²¿è·¯å¾„ '{path}' çš„ç»“æœã€‚"
        
        output = [f"ğŸ”— **å¤šè·³æŸ¥è¯¢ç»“æœ** (èµ·ç‚¹: {start_entity}, è·¯å¾„: {path})\n"]
        output.append(f"æ‰¾åˆ° {len(paths)} æ¡è·¯å¾„:\n")
        
        for i, p in enumerate(paths, 1):
            # è·å–æ¯ä¸ªèŠ‚ç‚¹çš„åç§°
            path_names = []
            for node_id in p:
                entity = store.get_entity(node_id)
                path_names.append(entity.name if entity else node_id)
            
            # æ·»åŠ è°“è¯
            formatted_path = []
            for j, name in enumerate(path_names):
                formatted_path.append(name)
                if j < len(predicates):
                    formatted_path.append(f"-[{predicates[j]}]->")
            
            output.append(f"  {i}. {''.join(formatted_path)}")
        
        return "\n".join(output)
        
    except Exception as e:
        return f"âŒ å¤šè·³æŸ¥è¯¢å¤±è´¥: {str(e)}"
